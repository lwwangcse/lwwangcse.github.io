<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Layout and HTML pieces the work of Jon Barron http://www.cs.berkeley.edu/~barron/ */
      a {
      color: #1772d1;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09227;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      tr {
        padding: 20px;
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700;
      }
      #teaser-td {
      text-align: center;
      margin: auto;
      display: block;
      }
      #teaser {
      width: 200px;
      border-style: none;
      margin-left: auto;
      margin-right: auto;
      }
      hr {
      border: 1px solid black;
      }
    </style>
    <link href="./css" rel="stylesheet" type="text/css">
    <title>Liwei Wang</title>
    

    <script async="" src="./analytics.js"></script><script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-34477541-5', 'auto');
      ga('send', 'pageview');
    </script>

  </head>

  <body>
    <table width="900" border="0" align="center" cellpadding="20">
      <tbody><tr>
	      <td>

          <table id="intro" width="100%" align="center" border="0" cellpadding="10">
            <tbody><tr>
              <td width="65%" valign="middle">
		            <p align="center"><font size="6">Liwei Wang</font></p>
		            <p align="justify">
                  I am an Assistant Professor in Computer Science and Engineering department at The Chinese University of Hong Kong (CUHK). Before coming to HK, I have worked for more than two years as a Senior Researcher in Tencent America at Bellevue, US. <br><br>

                  I got my PhD from Computer Science Department, University of Illinois at Urbana-Champaign, advised by Prof. <a target="_blank" href="https://slazebni.cs.illinois.edu/">Svetlana Lazebnik</a>. The Language and Vision (LaVi) Lab, which I founded at the Department of Computer Science and Engineering at CUHK, conducts research in Language+Vision, i.e. the intersection of language and vision.
		  <br><br>	    
		  If you want to join LaVi Lab, please send an email to lwwang@cse.cuhk.edu.hk 

		            </p>
		            <p align="center">
		              <a target="_blank" href="mailto:lwwang@cse.cuhk.edu.hk">Email</a> /
                  <a target="_blank" href="https://scholar.google.com/citations?user=qnbdnZEAAAAJ&hl=en">Google Scholar</a> /
				    <a target="_blank" href="./fulllist.html">Publications</a> /
				    <a target="_blank" href="./">Lab website (soon)</a> /
		            </p>
              </td>
              <td width="70%">
                <br><br>
  		          <img width="75%" src="./liwei.png" alt="A M">
	           </td>
            </tr>
          </tbody></table>

          <table id="heading" width="900" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="70%" valign="middle">
                <font size="5">News</font>

                <ul>
                <li>  <div style="display: inline;text-align:left;"> <div style="background-color: rgba(255, 0, 0, 0.4); display: inline;"> We are hiring Researchers, Interns, Postdocs and Phd students to work on topics of Embodied AI, Vision+Language, LLMs, and multi-modal LLMs.</div> </div></li>

		<li>2025/02: <div style="display: inline;text-align:left;"> Will serve as an Area Chair of NeurIPS 2025. </div></li>

		<li>2025/02: <div style="display: inline;text-align:left;"> Will serve as an Area Chair of ACL ARR 2025. </div></li>

		<li>2024/12: <div style="display: inline;text-align:left;"> Will serve as an Area Chair of ICML 2025. </div></li>
		
		<li>2024/09: <div style="display: inline;text-align:left;"> Will serve as an Area Chair of CVPR 2025. </div></li>

		<li>2024/05: <div style="display: inline;text-align:left;"> Will serve as an Area Chair of NeurIPS 2024. </div></li>
		
		<li>2023/10: <div style="display: inline;text-align:left;"> Our work on LLMs Preference Model have been accepted to EMNLP 2023! </div></li>

		<li>2023/10: <div style="display: inline;text-align:left;"> Our CLEVA, the comprehevsive Chinese Language Model Evalution Platform, has been accepted to EMNLP 2023 System Demonstration! </div></li>

		<li>2023/07: <div style="display: inline;text-align:left;"> I was invited to give a talk in the IAS Workshop on Mathematical Theory for Emergent Intelligence. </div></li>
		
		<li>2023/07: <div style="display: inline;text-align:left;"> Our work on Vision-Language Parameter-Efficient Tuning has been accepted to ICCV 2023. </div></li>
			
		<li>2023/07: <div style="display: inline;text-align:left;"> Our work on reasoning with LLMs has been accepted to ACL 2023. </div></li>
			
		<li>2023/06: <div style="display: inline;text-align:left;"> Will serve as an Area Chair of CVPR 2024 </div></li>
		
		<li>2022/10: <div style="display: inline;text-align:left;"> LaVi's two EMNLP 2022 Long papers on Dialogue research are accepted! </div></li>
		
		<li>2022/10: <div style="display: inline;text-align:left;"> Serving as an Area Chair of CVPR 2023 </div></li>

		<li>2022/07: <div style="display: inline;text-align:left;"> Our LaVi team won the annual NLP challenge LIC 2022 (Multi-modal Video Understanding track) hosted by CCF and CIPS, check department news <a target="_blank" href="https://www.cse.cuhk.edu.hk/news/achievements/wang-team-first-place-lic-2022-challenge/"> here </a>. </div></li>
		
		<li>2022/07: <div style="display: inline;text-align:left;"> One ECCV paper is accepted </div></li>

		<li>2022/03: <div style="display: inline;text-align:left;"> Three papers got accepted to CVPR 2022 including our new work on Language + Vision.  </div></li>

		<li>2022/02: <div style="display: inline;text-align:left;"> One ACL 2022 long paper from our group on "Probing Pre-trained Models" has bee accepted. </div></li>
			
		<li>2022/02: <div style="display: inline;text-align:left;"> Serving as an Area Chair of ECCV 2022 .  </div></li>

		<!--<li>2021/12: <div style="display: inline;text-align:left;"> I was invited to give an online talk at Microsoft Vision-and-Language Research Talk Series.  </div></li>
			
		<li>2021/12: <div style="display: inline;text-align:left;"> I am teaching <a target="_blank" href="https://lavi-lab.github.io/csci3320/"> CSCI 3320 Fundamentals of Machine Learning </a> at CUHK.  </div></li>
		<li>2021/08: <div style="display: inline;text-align:left;"> I am teaching <a target="_blank" href="https://lavi-lab.github.io/csci5640/"> CSCI 5640 NLP </a> at CUHK.  </div></li>
		
		<li>2021/08: <div style="display: inline;text-align:left;"> Our long paper on Dialog Rewriting was accepted to EMNLP 2021. </div></li>
		<li>2021/08: <div style="display: inline;text-align:left;"> I will teach a graduate level new course in CUHK - CSCI 5640 NLP in the coming semester. </div></li>
		<li>2021/08: <div style="display: inline;text-align:left;"> Three papers got accepted to ICCV 2021, including one Oral on Language + Vision. </div></li>
			
		<li>2021/07: <div style="display: inline;text-align:left;"> Our collaborated work had won the referit3d <a target="_blank" href="https://referit3d.github.io/benchmarks.html"> CVPR 2021 challenge </a>! </div></li>
		<li>2021/05: <div style="display: inline;text-align:left;"> I will teach a new NLP course in CSE at CUHK next semester. </div></li>
		<li>2021/05: <div style="display: inline;text-align:left;"> New work on text generation accepted to ACL 2021. </div></li>
		
		<li>2021/04: <div style="display: inline;text-align:left;"> I am joining the Editorial Board of IJCV - top computer vision journal in the world (CCF A journal in AI). </div></li>
		
		<li>2021/04: <div style="display: inline;text-align:left;"> I will serve as the program committee of EMNLP 2021. </div></li>
		
		<li>2021/04: <div style="display: inline;text-align:left;"> CUHK CSE 2022 Fall Early Admission starts! For Phd/Master applicants, please <a target="_blank" href="https://www.cse.cuhk.edu.hk/admission/mphil-phd-in-computer-science-and-engineering/early-admission/">click</a> . </div></li>
		
		<li>2021/03: <div style="display: inline;text-align:left;"> Our work on Logical Reasoning has been accepted by NAACL 2021. Congrats to my intern. </div></li>
		<li>2021/03: <div style="display: inline;text-align:left;"> Four CVPR 2021 papers are accepted including one oral paper. </div></li>
                <li>2020/12: <div style="display: inline;text-align:left;"> I joined the CSE@CUHK as an assistant professor and started my team working on Language and Vision ("LaVi" Lab). </div></li>
                <li>2020/11: <div style="display: inline;text-align:left;">  Our "Arrival" team is ranked the 1st among almost 400 teams in 2020 BAAI-JD Multimodal Dialogue Challenge!</div></li>
                <!--<li>2020/07: <div style="display: inline;text-align:left;">Two ECCV 2020 papers on vision+language are accepted! Congrats to my interns at Bellevue!</div></li>
                <li>2020/06: <div style="display: inline;text-align:left;">I am co-organizing the second Learning from Imperfect Data workshop in CVPR 2020.</div></li>
                <li>2020/04: <div style="display: inline;text-align:left;">To generate coherent video paragraphs? Take a look at our ACL 2020 long paper-MART.</div></li> -->

                </ul>


              </td>
            </tr>
          </tbody></table>

          <table id="heading" width="900" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="60%" valign="middle">
                <font size="5"> Recent Research Highlights</font>
                <p align="margin-left">
                  My students / interns / postdocs are indicated by '*'. Click <a target="_blank" href="./fulllist.html">full publication list</a>

                </p>

                </p>

              </td>
            </tr>
          </tbody></table>

     <table id="workworkwork" width="100%" align="center" cellpadding="20">

	<tr id="vgllm2025">
                <td width="25%" valign="top">
                  <img id="teaser" src="./vgllm.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2505.24625"> Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors </a></b><br>
                  <p></p>
                        Duo Zheng*, Shijia Huang*, Yanyang Li, <strong>Liwei Wang</strong><br> Arxiv 2025 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/VG-LLM">Code</a>  <br>
                </td>
            </tr>

	<tr id="acl2025">
                <td width="25%" valign="top">
                  <img id="teaser" src="./acl2025.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2502.15771"> Learning to Reason from Feedback at Test-Time </a></b><br>
                  <p></p>
                        Yanyang Li*, Michael R. Lyu, <strong>Liwei Wang</strong><br> ACL 2025 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/FTTT">Code</a>  <br>
                </td>
            </tr>

	<tr id="acl2025finding">
                <td width="25%" valign="top">
                  <img id="teaser" src="./acl2025finding.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2412.04947"> C<sup>2</sup>LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation
 </a></b><br>
                  <p></p>
                        Yanyang Li, Tin Long Wong, Cheung To Hung, Jianqiao Zhao, Duo Zheng, Ka Wai Liu, Michael R. Lyu, <strong>Liwei Wang</strong><br> ACL 2025 Findings &nbsp &nbsp <a target="_blank" href="http://www.lavicleva.com/c2leva/">Project</a>  <br>
                </td>
            </tr>


	<tr id="video3D_2024">
                <td width="25%" valign="top">
                  <img id="teaser" src="./video3d.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2412.00493"> Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding </a></b><br>
                  <p></p>
                        Duo Zheng*, Shijia Huang*, <strong>Liwei Wang</strong><br> CVPR 2025 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/Video-3D-LLM">Code</a>  <br>
                </td>
            </tr>

	<tr id="arxiv_2024">
                <td width="25%" valign="top">
                  <img id="teaser" src="./aim.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2412.03248"> AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning </a></b><br>
                  <p></p>
                        Yiwu Zhong*, Zhuoming Liu, Yin Li, <strong>Liwei Wang</strong><br> ICCV 2025 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/AIM">Code</a>  <br>
                </td>
            </tr>

	<tr id="iccv_2025_video">
                <td width="25%" valign="top">
                  <img id="teaser" src="./iccv2025_video.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://www.arxiv.org/pdf/2508.00518">Fine-grained Spatiotemporal Grounding on Egocentric Videos</a></b><br>
                  <p></p>
                        Shuo Liang, Yiwu Zhong, Zi-Yuan Hu, Yeyao Tao, <strong>Liwei Wang</strong><br> ICCV 2025 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/EgoMask">Code</a>  <br>
                </td>
            </tr>
	<tr id="arxiv_2023">
                <td width="25%" valign="top">
                  <img id="teaser" src="./navillm.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2312.02010.pdf">Towards Learning a Generalist Model for Embodied Navigation </a></b><br>
                  <p></p>
                        Duo Zheng*, Shijia Huang*, Lin Zhao, Yiwu Zhong, <strong>Liwei Wang</strong><br> CVPR 2024 (Poster Highlight) &nbsp &nbsp <a target="_blank" href="https://github.com/zd11024/NaviLLM">Code</a>  <br>
                </td>
            </tr>

	<tr id="ijcv_2024">
                <td width="25%" valign="top">
                  <img id="teaser" src="./ijcv2024.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="">A Mutual Supervision Framework for Referring Expression Segmentation and Generation</a></b><br>
                  <p></p>
                        Shijia Huang*, Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, <strong>Liwei Wang</strong><br> IJCV 2024 &nbsp &nbsp   <br>
                </td>
            </tr>

	     
	<tr id="acl_2024">
                <td width="25%" valign="top">
                  <img id="teaser" src="./acl2024.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2408.03246"> Making Long-Context Language Models Better Multi-Hop Reasoners </a></b><br>
                  <p></p>
                        Yanyang Li*, Shuo Liang*, Michael R. Lyu, <strong>Liwei Wang</strong><br> ACL 2024 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/LongContextReasoner">Code</a>  <br>
                </td>
            </tr>

	     
	<tr id="arxiv_2024">
                <td width="25%" valign="top">
                  <img id="teaser" src="./visualtable.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2403.18252.pdf"> Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models </a></b><br>
                  <p></p>
                        Yiwu Zhong*, Ziyuan Hu*, Michael R. Lyu, <strong>Liwei Wang</strong><br> EMNLP 2024 &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/Visual-Table">Code</a>   <br>
                </td>
            </tr>


	<tr id="emnlp2024video">
                <td width="25%" valign="top">
                  <img id="teaser" src="./emnlp2024video.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2410.05714">Enhancing Temporal Modeling of Video LLMs via Time Gating </a></b><br>
                  <p></p>
                        Zi-Yuan Hu*, Yiwu Zhong, Shijia Huang, Michael R. Lyu, <strong>Liwei Wang</strong><br> EMNLP 2024 Findings &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/TG-Vid">Code</a>  <br>
                </td>
            </tr>
	
	     
	<tr id="emnlp_2023">
                <td width="25%" valign="top">
                  <img id="teaser" src="./emnlp_2023.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://aclanthology.org/2023.emnlp-main.570.pdf">Learning Preference Model for LLMs via Automatic Preference Data Generation</a></b><br>
                  <p></p>
                        Shijia Huang*, Jianqiao Zhao, Yanyang Li, <strong>Liwei Wang</strong><br> EMNLP 2023 Long Paper <br>
                </td>
            </tr>
	     
           <tr id="cleva_2023">
                <td width="25%" valign="top">
                  <img id="teaser" src="./cleva_2023.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/pdf/2308.04813.pdf">CLEVA: Chinese Language Models EVAluation Platform</a></b><br>
                  <p></p>
                        Yanyang Li*, Jianqiao Zhao, Duo Zheng, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael R. Lyu,
			<strong>Liwei Wang</strong><br> EMNLP 2023 System Demonstration &nbsp &nbsp <a target="_blank" href="https://github.com/LaVi-Lab/CLEVA">Project</a> <br>
                </td>
            </tr>

	   <tr id="iccv_2023">
                <td width="25%" valign="top">
                  <img id="teaser" src="./iccv_2023.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="">VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control</a></b><br>
                  <p></p>
                        Ziyuan Hu*,
                        Yanyang Li*, Michael R. Lyu,
			<strong>Liwei Wang</strong><br>
                        ICCV, 2023 &nbsp &nbsp <a target="_blank" href="https://github.com/HenryHZY/VL-PET">Code</a> <br>
                </td>
            </tr>
	     
	   <tr id="cvpr3Dground_2022">
                <td width="25%" valign="top">
                  <img id="teaser" src="./cvpr3Dground_2022.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Multi-View Transformer for 3D Visual Grounding</a></b><br>
                  <p></p>
                        Shijia Huang*,
                        Yilun Chen, Jiaya Jia,
			<strong>Liwei Wang</strong><br>
                        CVPR, 2022 &nbsp &nbsp <a target="_blank" href="https://github.com/sega-hsj/MVT-3DVG">Code</a> <br>
                </td>
            </tr>
	   	<tr id="largemodel_2022">
                <td width="25%" valign="top">
                  <img id="teaser" src="./largemodel2022.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation</a></b><br>
                  <p></p>
                        Yanyang Li*,
			Jianqiao Zhao*,
			Michael R. Lyu,
			<strong>Liwei Wang</strong><br>
                        EMNLP, 2022 Long Paper, <a target="_blank" href="https://github.com/lyy1994/PLM_as_KB/tree/main/projects/plm_as_kb">Code</a> <br>
                </td>
            </tr>

	    <tr id="eval_2022">
                <td width="25%" valign="top">
                  <img id="teaser" src="./eval2022.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2202.06633">FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows</a></b><br>
                  <p></p>
                        Jianqiao Zhao*,
                        Yanyang Li*,
			Wanyu Du*,
			Yangfeng Ji,
			Dong Yu,
			Michael R. Lyu,
			<strong>Liwei Wang</strong><br>
                        EMNLP, 2022 Long Paper, <a target="_blank" href="https://github.com/Jianqiao-Zhao/FlowEval">Code and Dataset</a> <br>
                </td>
            </tr>

	   <tr id="acl_2022">
                <td width="25%" valign="top">
                  <img id="teaser" src="./acl2022.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency</a></b><br>
                  <p></p>
                        Yanyang Li*,
                        Fuli Luo,
			Runxin Xu,
			Songfang Huang,
			Fei Huang,
			<strong>Liwei Wang</strong><br>
                        ACL, 2022, long paper &nbsp &nbsp <br>
                </td>
            </tr>
	     
	    <tr id="iccv_2021">
                <td width="25%" valign="top">
                  <img id="teaser" src="./iccv2021.png">
                </td>
                <td width="55%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2105.11450">SAT: 2D Semantics Assisted Training for 3D Visual Grounding</a></b><br>
                  <p></p>
                        Zhengyuan Yang,
                        Songyang Zhang,
			<strong>Liwei Wang</strong>,
			Jiebo Luo <br>
                        ICCV, 2021, Oral Presentation &nbsp &nbsp <a target="_blank" href="https://github.com/zyang-ur/SAT">Code</a> <br>
                </td>
            </tr>

            <tr id="arxiv_2020">
                <td width="25%" valign="top">
                  <img id="teaser" src="./weak_grounding.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2007.01951">Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation</a></b><br>
                  <p></p>
                        <strong>Liwei Wang</strong>,
                        Jing Huang,
                        Yin Li,
                        Kun Xu,
                        Zhengyuan Yang,
                        Dong Yu<br>
                        CVPR, 2021 &nbsp &nbsp <a target="_blank" href="https://github.com/JingHuang81/weak-sup-visual-grounding">Code</a> <br>
                </td>
            </tr>
	    
	    <tr id="emnlp2021">
                <td width="25%" valign="top">
                  <img id="teaser" src="./emnlp2021.png">
                </td>
                <td width="35%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2012.14535"> Robust Dialogue Utterance Rewriting as Sequence Tagging </a></b><br>
                  <p></p>
                        Jie Hao,
                        Linfeng Song,
			<strong>Liwei Wang</strong>,
			Kun Xu,
			Zhaopeng Tu,
			Dong Yu <br>
                        EMNLP, 2021, &nbsp &nbsp <a target="_blank" href="https://github.com/freesunshine0316/RaST-plus">Code</a> <br>
                </td>
            </tr>

            <tr id="eccv_2020">
                <td width="25%" valign="top">
                  <img id="teaser" src="./captionECCV.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2007.11731">Comprehensive Image Captioning via Scene Graph Decomposition</a></b><br>
                  <p></p>
                        Yiwu Zhong*,
                        <strong>Liwei Wang</strong>,
                        Jianshu Chen,
                        Dong Yu,
                        Yin Li<br>
                        <i>ECCV</i>, 2020  &nbsp &nbsp <a target="_blank" href="https://github.com/YiwuZhong/Sub-GC">Code</a> <br>
                </td>
            </tr>


            <tr id="eccv_2020">
                <td width="25%" valign="top">
                  <img id="teaser" src="./improveECCV.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2008.01059">Improving One-stage Visual Grounding by Recursive Sub-query Construction</a></b><br>
                  <p></p>
                        Zhengyuan Yang,
                        Tianlang Chen,
                        <strong>Liwei Wang</strong>,
                        Jiebo Luo<br>
                        <i>ECCV</i>, 2020 &nbsp &nbsp <a target="_blank" href="https://github.com/zyang-ur/ReSC">Code</a> <br>
                </td>
            </tr>


            <tr id="acl_2020">
                <td width="25%" valign="top">
                  <img id="teaser" src="./acl2020.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/2005.05402">MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning</a></b><br>
                  <p></p>
                        Jie Lei*,
                        <strong>Liwei Wang</strong>,
                        Yelong Shen,
                        Dong Yu,
                        Tamara Berg,
                        Mohit Bansal<br>
                        <i>ACL</i>, 2020 &nbsp &nbsp <a target="_blank" href="https://github.com/jayleicn/recurrent-transformer">Code</a>   <br>
                </td>
            </tr>


            <tr id="iccv_2019">
                <td width="25%" valign="top">
                  <img id="teaser" src="./iccv2019.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/1908.06354">A Fast and Accurate One-Stage Approach to Visual Groundingâ€‹</a></b><br>
                  <p></p>
                        Zhengyuan Yang*,
                        Boqing Gong,
                        <strong>Liwei Wang</strong>,
                        Wenbing Huang,
                        Dong Yu,
                        Jiebo Luo<br>
                        <i>ICCV</i>, 2019, Oral Presentation &nbsp &nbsp <a target="_blank" href="https://github.com/zyang-ur/onestage_grounding">Code</a> <br>
                </td>
            </tr>

            <tr id="CVPR2019">
                <td width="25%" valign="top">
                  <img id="teaser" src="./cvpr2019.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/1805.12589">Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech</a></b><br>
                  <p></p>
                        Aditya Deshpande, 
                        Jyoti Aneja, 
                        <strong>Liwei Wang</strong>,
                        Alexander Schwing, 
                        D. A. Forsyth<br>
                        <i>CVPR</i>, 2019, Oral Presentation<br>
                </td>
            </tr>


            <tr id="PAMI">
                <td width="25%" valign="top">
                  <img id="teaser" src="./pami.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="https://arxiv.org/abs/1704.03470">Learning Two-Branch Neural Networks for Image-Text Matching Tasks</a></b><br>
                  <p></p>
                        <strong>Liwei Wang</strong>,
                        Yin Li,
                        Jing Huang,
                        Svetlana Lazebnik<br>
                        <i>TPAMI</i>, 2018  &nbsp &nbsp <a target="_blank" href="https://github.com/lwwang/Two_branch_network">Code</a>  <br>
                </td>
            </tr>


            <tr id="protein">
                <td width="25%" valign="top">
                  <img id="teaser" src="./protein.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Learning structural motif representations for efficient protein structure search</a></b><br>
                  <p></p>
                        Yang Liu,
                        Qing Ye,
                        <strong>Liwei Wang</strong>, 
                        Jian Peng<br>
                        <i>Bioinformatics</i>, 2018  &nbsp &nbsp <a target="_blank" href="https://github.com/largelymfs/DeepFold">Code</a> <br>   
                </td>
            </tr>



            <tr id="nips">
                <td width="25%" valign="top">
                  <img id="teaser" src="./nips2017.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space</a></b><br>
                  <p></p>
                        <strong>Liwei Wang</strong>, 
                        Alex Schwing,
                        Svetlana Lazebnik<br>
                        <i>NeurIPS</i>, 2017<br>
                </td>
            </tr>

            <tr id="nips">
                <td width="25%" valign="top">
                  <img id="teaser" src="./IJCV.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models</a></b><br>
                  <p></p>
                        Bryan Plummer,
                        <strong>Liwei Wang</strong>,
                        Chris M. Cervantes, 
                        Juan C. Caicedo, 
                        Julia Hockenmaier, 
                        Svetlana Lazebnik<br>
                        <i>IJCV</i>, 2016      &nbsp &nbsp <a target="_blank" href="http://bryanplummer.com/Flickr30kEntities/">Project</a>          <br>   
                </td>
            </tr>



            <tr id="nips">
                <td width="25%" valign="top">
                  <img id="teaser" src="./cvpr2016.png">
                </td>
                <td width="75%" valign="center">
                  <p>
                        <b><a target="_blank" href="">Learning Deep Structure-Preserving Image-Text Embeddings</a></b><br>
                  <p></p>
                        <strong>Liwei Wang</strong>,
                        Yin Li,
                        Svetlana Lazebnik<br>
                        <i>CVPR</i>, 2016  &nbsp &nbsp <a target="_blank" href="https://drive.google.com/file/d/0B_s0NuQ5DMW_LVd1VmNfTDBORGM/view">Code</a>   <br>       
                </td>
            </tr>


          </tbody></table>


          <table id="thanks" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <br>
                <p align="right"><font size="2">
                  <a href="http://arunmallya.com/">(Templates adapted from Arun's webpage.)</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </tbody></table>
  

</body></html>
